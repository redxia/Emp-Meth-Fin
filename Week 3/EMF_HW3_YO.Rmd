---
title: "Empirical Methods HW 2"
author: 'Group 9: Linqi Huang, Abhesh Kumar, Yu Onohara, Maitrayee Patil, Redmond Xia'
date: "January 19, 2020"
output: pdf_document
---

## Problem 1

1.

The first order autocorrelation of the ARMA(1,1) model can be written as follow.

\begin{align*}
\rho_1 &= \phi_1 - \theta_1 \frac{\sigma_\epsilon^2}{\gamma_0}\\
\end{align*}

Hence, we will compute $\gamma_0$ first, and then also compute $\rho_1$.

```{r}
# Assumptions
phi1 = 0.95
theta1 = 0.9
sigma = 0.05

# Caluculating the covariance with lag 0 (= Variance of yt)
gamma0 = sigma^2 * (1 + theta1^2 - 2 * phi1 * theta1) / (1 - phi1^2)
gamma0

# Using gamma above, compute the first order autocorrelation
rho1 = phi1 - theta1 * (sigma^2 / gamma0)
rho1
```

2.

The $j$th ($j > 1$) order autocorrelation of the ARMA(1,1) model can be written as follow.

\begin{align*}
\rho_j = \phi_1\rho_(j-1)
\end{align*}

Therefore, we will compute the second order autocorrelation of the ARMA model as follow.

```{r}
# Using the first order autocorrelation, compute the second order autocorrelation
rho2 = phi1 * rho1
rho2
rho2 / rho1 # = phi1
```

The ratio of the second-order to the first-order autocorrelation equals to $\phi_1$. since this fact implies that the autocorrelation of the this model $(\phi_1 < 1)$ converge to 0 $(\lim_{j \to \infty} \rho_j = 0)$, this process is stationary.

3.

We will compute the conditional expectation of $y_{t+1}$ and $y_{t+2}$ as follow.

\begin{align*}
E[y_{t+1}|y_t] &= E[\phi_1 y_t - \theta_1 \epsilon_t + \epsilon_{t+1}] \\
&= \phi_1 y_t - \theta_1 \epsilon_t + 0 \\
&= \phi_1 y_t - \theta_1 \epsilon_t 
\end{align*}

\begin{align*}
E[y_{t+2}|y_t] &= E[\phi_1 y_{t+1} - \theta_1 \epsilon_{t+1} + \epsilon_{t+2}] \\
&= \phi_1 E[y_{t+1}|Y_t] - \theta_1 E[\epsilon_{t+1}] + E[\epsilon_{t+2}] \\
&= \phi_1 (\phi_1 y_t - \theta_1 \epsilon_t) \\
&= \phi_1^2 y_t - \phi_1 \theta_1 \epsilon_t
\end{align*}
```{r}
# Calculatingt the conditional expectations of yt+1 and yt+2
yt = 0.6
epsiront = 0.1
E_yt1 = phi1 * yt - theta1 * epsiront
E_yt2 = phi1^2 * yt - phi1 * theta1 * epsiront
E_yt1
E_yt2
```

4.


## Problem 2

1.

From the assumption $\phi = 0$, we can rewrite the $e_t$ as follow.

$$e_t = e_{t-1} + \epsilon_t$$

Then, using the recursive calculation of $e_t$, we can obtain $e_{t-4}$ as follow.

\begin{align*}
e_{t-4} &= e_{t-3} - \epsilon_{t-3} \\
e_{t-3} &= e_{t-2} - \epsilon_{t-2} \\
e_{t-2} &= e_{t-1} - \epsilon_{t-1} \\
\end{align*}

Hence,
$$e_{t-4} = e_{t-1} - \epsilon_{t-1} - \epsilon_{t-2} - \epsilon_{t-3}$$

Therefore,

\begin{align*}
y_t &= e_{t} - e_{t-4} \\
y_t &= e_{t-1} + \epsilon{t} - (e_{t-1} - \epsilon_{t-1} - \epsilon_{t-2} - \epsilon_{t-3}) \\
y_t &= \epsilon_t + \epsilon_{t-1} + \epsilon_{t-2} + \epsilon_{t-3} 
\end{align*}

To calculate the autocovariances of this process, we will use the following property of the covariance.

$$Cov(aX + bY, cW + dV) = acCov(X, W) + adCov(X,Z) + bcCov(Y, W) + bdCov(Y,V)$$

Since $\epsilon_t$ is i.i.d, $Cov(\epsilon_t, \epsilon_{t-j}) = 0$ if any $j > 0$.
Therefore, we can calculate the autocovariance of order 0 through 5 as follow.

\begin{align*}
Cov(y_t, y_t) &= Var(\epsilon_{t}) + Var(\epsilon_{t-1}) + Var(\epsilon_{t-2})+ Var(\epsilon_{t-3}) \\
&= 4\sigma^2 \\
Cov(y_t, y_{t-1}) &= Var(\epsilon_{t-1}) + Var(\epsilon_{t-2})+ Var(\epsilon_{t-3}) \\
&= 3\sigma^2 \\
Cov(y_t, y_{t-2}) &= Var(\epsilon_{t-2})+ Var(\epsilon_{t-3}) \\
&= 2\sigma^2 \\
Cov(y_t, y_{t-3}) &= Var(\epsilon_{t-3}) \\
&= \sigma^2 \\
Cov(y_t, y_{t-4}) &= 0 \\
Cov(y_t, y_{t-5}) &= 0
\end{align*}

3.

This model has no AR structure because we have no $y_{t-j}$ terms in the model. However, $\epsilon_t$ has 3 $\epsilon$ terms. Hence, this model is ARMA(0,4) model (MA(4) model). Also, each coefficient of $\epsilon$ terms equals to 1.