---
title: "Empirical Methods HW3"
author: 'Group 9: Linqi Huang, Abhesh Kumar, Yu Onohara, Maitrayee Patil, Redmond Xia'
date: "January 27, 2020"
header-includes:
- \usepackage{amssymb, amsmath, amsthm}
- \usepackage{tabu}
- \usepackage{amsmath}
- \newcommand{\E}{\mathbb{E}}
- \newcommand{\var}{{\rm Var}}
- \newcommand{\N}{\mathcal{N}}
- \newcommand{\cov}{\mathrm{Cov}}
output: pdf_document
---
# Problem 2.1
$e_t = e_{t-1} + x_t \implies x_t = e_t - e_{t-1}$
$x_t = \phi x_{t-1} + \epsilon_t$
$y_t \equiv e_t - e_{t-4}$


\begin{align*}
x_t = e_t - e_{t-1} = \phi x_{t-1} + \epsilon_t =\epsilon_t,\ for\ \phi = 0 \\
x_t = \epsilon_t = e_t - e_{t-1} \\
\epsilon_{t-1} = e_{t-1} - e_{t-2} \\
\epsilon_{t-2} = e_{t-2} - e_{t-3} \\
\epsilon_{t-3} = e_{t-3} - e_{t-4} \\
y_t = \epsilon_{t} + \epsilon_{t-1} + \epsilon_{t-2} + \epsilon_{t-3} = e_{t} - e_{t-4}
\end{align*}




\begin{align*}
E[y_t] = 0 + 0 + 0 + 0 = 0,\ Find\ Cov(y_t,y_{t-j})\ for\ j = 0,1,2,3,4,5 \\
j= 0, Cov(y_t,y_{t}) = Var(y_t) = \epsilon_{t}^2 + \epsilon_{t-1}^2 + \epsilon_{t-2}^2 + \epsilon_{t-3}^2 = 1 + 1 + 1 + 1 = 4 \\ \\
j = 1,Cov(y_t,y_{t - 1}) = E[y_t y_{t-1}] - E[y_t]E[y_{t-1}] = E[y_t y_{t-1}] - 0 = \\ 
E[(\epsilon_{t} + \epsilon_{t-1} + \epsilon_{t-2} + \epsilon_{t-3})(\epsilon_{t-1} + \epsilon_{t-2} + \epsilon_{t-3} + \epsilon_{t-4})],\ E[\epsilon_i \epsilon_j] = 0\  \forall i \ne j \\ \implies E[\epsilon_{t-1}^2 + \epsilon_{t-2}^2 + \epsilon_{t-3}^2] = 1 + 1 + 1 = 3 \\ \\
Similarly,\ j = 2, Cov(y_t,y_{t-2}) = E[(\epsilon_{t} + \epsilon_{t-1} + \epsilon_{t-2} + \epsilon_{t-3})(\epsilon_{t-2} + \epsilon_{t-3} + \epsilon_{t-4} + \epsilon_{t-5})] \\
= E[\epsilon_{t-2}^2 + \epsilon_{t-3}^2] = 1 + 1 = 2 \\ \\
Again,\ j = 3,\ Cov(y_t,y_{t-3}) = E[\epsilon_{t-3}^2] = 1 \\ \\ 
j = 4, Cov(y_t,y_{t-4}) = 0,\ j = 5, Cov(y_t,y_{t-5}) = 0 \\
\end{align*}


# Problem 2.2
The autocovariance drops to 0 after 3 lags. So the for every value after 3 it isn't correlated to $y_t$. With $\phi = 0$ We get the AR(0), this is a MA(3) then. So we have a ARMA(0,3).

# Problem 3
1.
\begin{align*}
Var(R_{t+1}^e) = \beta^2 Var(x_t) + Var(\epsilon_{t+1}) = 1 * (0.05)^2 + 0.15^2 = 0.025 \\
SD(R_{t+1}^e) = \sqrt{0.025} = .15811 \\
\end{align*}

2.

\begin{align*}
R^2 = \rho^2 = (\dfrac{Cov(R_{t+1}^e,x_t)}{\sigma_{R_{t+1}^e}\sigma_{x_t}})^2 = (\dfrac{Cov(\beta x_t + \epsilon_{t-1},x_t)}{\sigma_{R_{t+1}^e}\sigma_{x_t}})^2 = (\dfrac{Var(x_t)}{\sigma_{R_{t+1}^e}\sigma_{x_t}})^2 = \dfrac{Var(x_t)}{Var(R_{t+1}^e)} = \dfrac{0.05^2}{0.025} = 0.1 
\end{align*}


3. 
Sharpe Ratio = $\dfrac{E[R_{t+1}^e]}{\sigma_{mkt}} = \dfrac{0.05}{\sqrt{0.025}} = 0.31622$

4.

\begin{align*}
\gamma = \dfrac{40}{9} 
&\\ 
\alpha_t = \dfrac{E[x_t]}{\gamma \sigma_t^2[R_{t+1}^e]} = \dfrac{E[x_t]}{\gamma (\sigma_t^2[x_t] + \sigma_t^2[\epsilon_{t+1}])} \\
So,\ if\ x = 0,\ then\ \alpha_t = \dfrac{0}{positive} = 0 \implies Sharpe\ Ratio = 0\\
if\ x = .1, \alpha_t = \dfrac{.1}{\dfrac{40}{9} (0.15^2)} = 1 \implies Sharpe\ Ratio = \dfrac{.1}{.15} = 0.67
\end{align*} 

5.
a) Below is the Expected value of the return
```{r setup}
prob <- 0.5
xt_1 <- 0
xt_2 <- 0.1
alphat_1 <- 0
alphat_2 <- 1
sigma_e <- 0.15
E_alpR <- prob*alphat_1*xt_1 + prob*alphat_2*xt_2
E_alpR
```

b) The output below is the unconditional standard deviation
```{r}
var_alpR <- prob*alphat_2^2*(xt_2^2+sigma_e^2) - (prob*alphat_2*xt_2)^2
sqrt(var_alpR)
```

c)Below is the sharpe value
```{r}
E_alpR / sqrt(var_alpR)
```

d) 
i.Below is the implied R^2
```{r}
xt_3 <- -0.05
xt_4 <- 0.15
E_xt <- prob*xt_3 + prob*xt_4
Var_xt <- prob*(xt_3-E_xt)^2 + prob*(xt_4-E_xt)^2
R_sq2 <- Var_xt / (Var_xt+sigma_e^2)
R_sq2
```

ii.
```{r}
gamma_t <- 40/9
alphat_3 <- xt_3 / (gamma_t*sigma_e^2)
alphat_4 <- xt_4 / (gamma_t*sigma_e^2)
E_alpR2 <- prob*alphat_3*xt_3 + prob*alphat_4*xt_4
var_alpR2 <- prob*alphat_3^2*(xt_3^2+sigma_e^2) + prob*alphat_4^2*(xt_4^2+sigma_e^2) - E_alpR2^2
E_alpR2 / sqrt(var_alpR2)
```

