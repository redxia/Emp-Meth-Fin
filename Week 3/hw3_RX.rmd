---
title: "Empirical Methods HW3"
author: 'Group 9: Linqi Huang, Abhesh Kumar, Yu Onohara, Maitrayee Patil, Redmond Xia'
date: "January 27, 2020"
header-includes:
- \usepackage{amssymb, amsmath, amsthm}
- \usepackage{tabu}
- \usepackage{amsmath}
- \newcommand{\E}{\mathbb{E}}
- \newcommand{\var}{{\rm Var}}
- \newcommand{\N}{\mathcal{N}}
- \newcommand{\cov}{\mathrm{Cov}}
output: pdf_document
---
# Problem 2.1
$e_t = e_{t-1} + x_t \implies x_t = e_t - e_{t-1}$
$x_t = \phi x_{t-1} + \epsilon_t$
$y_t \equiv e_t - e_{t-4}$


\begin{align*}
x_t = e_t - e_{t-1} = \phi x_{t-1} + \epsilon_t =\epsilon_t,\ for\ \phi = 0 \\
x_t = \epsilon_t = e_t - e_{t-1} \\
\epsilon_{t-1} = e_{t-1} - e_{t-2} \\
\epsilon_{t-2} = e_{t-2} - e_{t-3} \\
\epsilon_{t-3} = e_{t-3} - e_{t-4} \\
y_t = \epsilon_{t} + \epsilon_{t-1} + \epsilon_{t-2} + \epsilon_{t-3} = e_{t} - e_{t-4}
\end{align*}




\begin{align*}
E[y_t] = 0 + 0 + 0 + 0 = 0,\ Find\ Cov(y_t,y_{t-j})\ for\ j = 0,1,2,3,4,5 \\
j= 0, Cov(y_t,y_{t}) = Var(y_t) = \epsilon_{t}^2 + \epsilon_{t-1}^2 + \epsilon_{t-2}^2 + \epsilon_{t-3}^2 = 1 + 1 + 1 + 1 = 4 \\ \\
j = 1,Cov(y_t,y_{t - 1}) = E[y_t y_{t-1}] - E[y_t]E[y_{t-1}] = E[y_t y_{t-1}] - 0 = \\ 
E[(\epsilon_{t} + \epsilon_{t-1} + \epsilon_{t-2} + \epsilon_{t-3})(\epsilon_{t-1} + \epsilon_{t-2} + \epsilon_{t-3} + \epsilon_{t-4})],\ E[\epsilon_i \epsilon_j] = 0\  \forall i \ne j \\ \implies E[\epsilon_{t-1}^2 + \epsilon_{t-2}^2 + \epsilon_{t-3}^2] = 1 + 1 + 1 = 3 \\ \\
Similarly,\ j = 2, Cov(y_t,y_{t-2}) = E[(\epsilon_{t} + \epsilon_{t-1} + \epsilon_{t-2} + \epsilon_{t-3})(\epsilon_{t-2} + \epsilon_{t-3} + \epsilon_{t-4} + \epsilon_{t-5})] \\
= E[\epsilon_{t-2}^2 + \epsilon_{t-3}^2] = 1 + 1 = 2 \\ \\
Again,\ j = 3,\ Cov(y_t,y_{t-3}) = E[\epsilon_{t-3}^2] = 1 \\ \\ 
j = 4, Cov(y_t,y_{t-4}) = 0,\ j = 5, Cov(y_t,y_{t-5}) = 0 \\
\end{align*}


# Problem 2.2
The autocovariance drops to 0 after 3 lags. So the for every value after 3 it isn't correlated to $y_t$. With $\phi = 0$ We get the AR(0), this is a MA(3) then. So we have a ARMA(0,3).

# Problem 3
1.
\begin{align*}
Var(R_{t+1}^e) = \beta^2 Var(x_t) + Var(\epsilon_{t+1}) = 1 * (0.05)^2 + 0.15^2 = 0.025 = \sqrt{0.025} = .15811 \\
\end{align*}

2.

\begin{align*}
R^2 = \rho^2 = (\dfrac{Cov(R_{t+1}^e,x_t)}{\sigma_{R_{t+1}^e}\sigma_{x_t}})^2 = (\dfrac{Cov(\beta x_t + \epsilon_{t-1},x_t)}{\sigma_{R_{t+1}^e}\sigma_{x_t}})^2 = (\dfrac{Var(x_t)}{\sigma_{R_{t+1}^e}\sigma_{x_t}})^2 = \dfrac{Var(x_t)}{Var(R_{t+1}^e)} = \dfrac{0.05^2}{0.025} = 0.1 
\end{align*}


3. 
Sharpe Ratio = $\dfrac{E[R_{t+1}^e]}{\sigma_{mkt}} = \dfrac{0.05}{\sqrt{0.025}} = 0.31622$

4.

\begin{align*}
\gamma = \dfrac{40}{9} 
&\\ 
\alpha_t = \dfrac{E[x_t]}{\gamma \sigma_t^2[R_{t+1}^e]} = \dfrac{E[x_t]}{\gamma (\sigma_t^2[x_t] + \sigma_t^2[\epsilon_{t+1}])} \\
So,\ if\ x = 0,\ then\ \alpha_t = \dfrac{0}{positive} = 0 \implies Sharpe\ Ratio = 0\\
if\ x = .1, \alpha_t = \dfrac{.1}{\dfrac{40}{9} (0.15^2)} = 1 \implies Sharpe\ Ratio = \dfrac{.1}{.15} = \dfrac{2}{3} 
\end{align*} 

5.
a) Below is the Expected value of the return
```{r setup}
gamma <- 40/9
xt <- c(0,0.1)
cond_var <- .15^2 
E_x2 <- var(xt)/2 + 0.05^2
uncond_ret <- 0.5*xt[1] + 0.5*xt[2] 
alpha = (uncond_ret / (gamma * (cond_var + var(xt)/2)))
E_alpR <- alpha * uncond_ret
E_alpR
```

b) The output below is the unconditional standard deviation
```{r}
var_alpR <- (E_x2 + cond_var - uncond_ret^2) * alpha^2
sqrt(var_alpR)
```

c)Below is the sharpe value
```{r}
E_alpR / sqrt(var_alpR)
```

d) 
i.Below is the implied R^2
```{r}
x <- c(-0.05,.15)
E_x <- mean(x)
var_x <- var(x) / 2
var_R <- var_x + cond_var
E_xSq <- var_x + E_x^2
R_Sq <- var_x / var_R
R_Sq
```

ii.
```{r}
alpha1 = (E_x / (gamma * (cond_var + var_x)))
var_alpR1 <- (E_xSq + cond_var - E_x^2) * alpha1^2
(alpha1 * E_x)  / sqrt(var_alpR1)
```

